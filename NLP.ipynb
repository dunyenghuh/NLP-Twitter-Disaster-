{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'the_iliad.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-cfab75f05c2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#from chunk_counters import np_chunk_counter, vp_chunk_counter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"the_iliad.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'the_iliad.txt'"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag, RegexpParser\n",
    "from nltk.tokenize import word_tokenize \n",
    "#https://www.geeksforgeeks.org/nlp-how-tokenizing-text-sentence-words-works/\n",
    "#from chunk_counters import np_chunk_counter, vp_chunk_counter\n",
    "\n",
    "text = open(\"the_iliad.txt\",encoding='utf-8').read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tokenize_words'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ee859c908da9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRegexpParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtokenize_words\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_sentence_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunk_counters\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_chunk_counter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvp_chunk_counter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# import text of choice here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tokenize_words'"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag, RegexpParser\n",
    "from tokenize_words import word_sentence_tokenize\n",
    "from chunk_counters import np_chunk_counter, vp_chunk_counter\n",
    "\n",
    "# import text of choice here\n",
    "text = open(\"the_iliad.txt\",encoding='utf-8').read().lower()\n",
    "\n",
    "# sentence and word tokenize text here\n",
    "word_tokenized_text = word_sentence_tokenize(text)\n",
    "\n",
    "# store and print any word tokenized sentence here\n",
    "single_word_tokenized_sentence = word_tokenized_text[300]\n",
    "print(single_word_tokenized_sentence)\n",
    "\n",
    "\n",
    "# create a list to hold part-of-speech tagged sentences here\n",
    "pos_tagged_text = []\n",
    "\n",
    "# create a for loop through each word tokenized sentence here\n",
    "for sent in word_tokenized_text:\n",
    "  pos_tagged_text.append(pos_tag(sent))\n",
    "  # part-of-speech tag each sentence and append to list of pos-tagged sentences here\n",
    "single_pos_sentence = pos_tagged_text[25] \n",
    "print(single_pos_sentence)\n",
    "# store and print any part-of-speech tagged sentence here\n",
    "np_chunk_grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "\n",
    "# define noun phrase chunk grammar here\n",
    "np_chunk_parser = RegexpParser(np_chunk_grammar)\n",
    "\n",
    "# create noun phrase RegexpParser object here\n",
    "vp_chunk_grammar = \"VP: {<DT>?<JJ>*<NN><VB.*><RB.?>?}\"\n",
    "\n",
    "# define verb phrase chunk grammar here\n",
    "vp_chunk_parser = RegexpParser(vp_chunk_grammar)\n",
    "\n",
    "# create verb phrase RegexpParser object here\n",
    "np_chunked_text = []\n",
    "vp_chunked_text = []\n",
    "\n",
    "# create a list to hold noun phrase chunked sentences and a list to hold verb phrase chunked sentences here\n",
    "\n",
    "for sent in pos_tagged_text:\n",
    "  np_chunked_text.append(np_chunk_parser.parse(sent))\n",
    "  vp_chunked_text.append(vp_chunk_parser.parse(sent))\n",
    "\n",
    "\n",
    "# create a for loop through each pos-tagged sentence here\n",
    "\n",
    "# chunk each sentence and append to lists here\n",
    "most_common_np_chunks = np_chunk_counter(np_chunked_text)  \n",
    "print(most_common_np_chunks)  \n",
    "\n",
    "# store and print the most common NP-chunks here\n",
    "most_common_vp_chunks = vp_chunk_counter(vp_chunked_text)  \n",
    "print(most_common_vp_chunks)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'goldman_emma_raw'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7745c5d4a8c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoldman_emma_raw\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgoldman_docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhenson_matthew_raw\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhenson_docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwu_tingfang_raw\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwu_docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# import sklearn modules here:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'goldman_emma_raw'"
     ]
    }
   ],
   "source": [
    "from goldman_emma_raw import goldman_docs\n",
    "from henson_matthew_raw import henson_docs\n",
    "from wu_tingfang_raw import wu_docs\n",
    "# import sklearn modules here:\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "bow_vectorizer = CountVectorizer()\n",
    "\n",
    "# Setting up the combined list of friends' writing samples\n",
    "friends_docs = goldman_docs + henson_docs + wu_docs\n",
    "# Setting up labels for your three friends\n",
    "friends_labels = [1] * 154 + [2] * 141 + [3] * 166\n",
    "\n",
    "# Print out a document from each friend:\n",
    "friends_vectors= bow_vectorizer.fit_transform(friends_docs)\n",
    "\n",
    "mystery_postcard = \"\"\"\n",
    "My friend,\n",
    "From the 10th of July to the 13th, a fierce storm raged, clouds of\n",
    "freeing spray broke over the ship, incasing her in a coat of icy mail,\n",
    "and the tempest forced all of the ice out of the lower end of the\n",
    "channel and beyond as far as the eye could see, but the _Roosevelt_\n",
    "still remained surrounded by ice.\n",
    "Hope to see you soon.\n",
    "\"\"\"\n",
    "\n",
    "# Create bow_vectorizer:\n",
    "print(goldman_docs[27])\n",
    "# Define friends_vectors:\n",
    "\n",
    "# Define mystery_vector: \n",
    "\n",
    "mystery_vector = bow_vectorizer.transform([mystery_postcard])\n",
    "\n",
    "\n",
    "\n",
    "# Define friends_classifier:\n",
    "friends_classifier = MultinomialNB()\n",
    "# Train the classifier:\n",
    "friends_classifier.fit(friends_vectors, friends_labels)\n",
    "# Change predictions:\n",
    "predictions = friends_classifier.predict(mystery_vector)\n",
    "\n",
    "mystery_friend = predictions[0] if predictions[0] else \"someone else\"\n",
    "\n",
    "#Uncomment the print statement:\n",
    "print(\"The postcard was from {}!\".format(mystery_friend))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
